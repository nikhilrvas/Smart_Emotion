{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a4dda07-7d36-43df-bdfb-83c2e6d14ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "data = pd.read_csv(r'D:\\grp_project_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0810a147-b0ae-4d75-a583-32793f8acd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity</th>\n",
       "      <th>emotion</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>TEMP_1</th>\n",
       "      <th>EDA_1</th>\n",
       "      <th>BVP_1</th>\n",
       "      <th>HR_1</th>\n",
       "      <th>IBI_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Running</td>\n",
       "      <td>Energetic</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>31.45</td>\n",
       "      <td>0.481635</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>103.00</td>\n",
       "      <td>83.359375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eating</td>\n",
       "      <td>Content</td>\n",
       "      <td>00:30:00</td>\n",
       "      <td>31.45</td>\n",
       "      <td>0.595640</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>98.33</td>\n",
       "      <td>84.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sleeping</td>\n",
       "      <td>Calm</td>\n",
       "      <td>01:00:00</td>\n",
       "      <td>31.45</td>\n",
       "      <td>0.611011</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>96.00</td>\n",
       "      <td>103.296875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Walking</td>\n",
       "      <td>Nostalgic</td>\n",
       "      <td>01:30:00</td>\n",
       "      <td>31.45</td>\n",
       "      <td>0.627663</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>92.80</td>\n",
       "      <td>109.328125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Studying</td>\n",
       "      <td>Focused</td>\n",
       "      <td>02:00:00</td>\n",
       "      <td>31.45</td>\n",
       "      <td>0.672496</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>88.00</td>\n",
       "      <td>110.312500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   activity    emotion timestamp  TEMP_1     EDA_1  BVP_1    HR_1       IBI_1\n",
       "0   Running  Energetic  00:00:00   31.45  0.481635   -0.0  103.00   83.359375\n",
       "1    Eating    Content  00:30:00   31.45  0.595640   -0.0   98.33   84.125000\n",
       "2  Sleeping       Calm  01:00:00   31.45  0.611011   -0.0   96.00  103.296875\n",
       "3   Walking  Nostalgic  01:30:00   31.45  0.627663   -0.0   92.80  109.328125\n",
       "4  Studying    Focused  02:00:00   31.45  0.672496   -0.0   88.00  110.312500"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f613b3e7-b81c-454e-8c30-ae9952d73d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bd64424-8d5a-4a59-9357-38bdd0272a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting hours from the timestamp column.\n",
    "data['Hour'] = pd.to_datetime(data['timestamp']).dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d013e12e-2191-4b4f-be83-09abc35dc094",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segregating hours into night, morning, afternoon and evening.\n",
    "data['TimeOfDay'] = pd.cut(data['Hour'], bins=[-1, 6, 12, 18, 24], labels=['night', 'morning', 'afternoon', 'evening'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6ea9b4e-1be4-4ed0-88a0-7814f4777ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using labelEncoder to convert repeating strings to numerical representations.\n",
    "time_encoder = LabelEncoder()\n",
    "data['TimeOfDay'] = time_encoder.fit_transform(data['TimeOfDay'])\n",
    "activity_encoder = LabelEncoder()\n",
    "emotion_encoder = LabelEncoder()\n",
    "data['ActivitywithLE'] = activity_encoder.fit_transform(data['activity'])\n",
    "data['EmotionwithLE'] = emotion_encoder.fit_transform(data['emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89cd554d-906f-4823-9ee3-d9ccb60e7a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X contains 4 parameters - Daytime, Activity, Body temperature in celcius and electrodermal activity in microsiemens.\n",
    "X = data[['TimeOfDay', 'ActivitywithLE', 'TEMP_1', 'EDA_1','BVP_1','HR_1','IBI_1']]\n",
    "#y contains target column, Emotion.\n",
    "y = data['EmotionwithLE']\n",
    "\n",
    "#Splitting the data into train and test sets (80:20).\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaf5d4cf-aa01-4cea-ac99-81070e5273ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using StandardScaler to scale all parameters.\n",
    "scaler = StandardScaler()\n",
    "X_train[['TimeOfDay', 'ActivitywithLE', 'TEMP_1', 'EDA_1','BVP_1','HR_1','IBI_1']] = scaler.fit_transform(X_train[['TimeOfDay', 'ActivitywithLE', 'TEMP_1', 'EDA_1','BVP_1','HR_1','IBI_1']])\n",
    "X_test[['TimeOfDay', 'ActivitywithLE', 'TEMP_1', 'EDA_1','BVP_1','HR_1','IBI_1']] = scaler.transform(X_test[['TimeOfDay', 'ActivitywithLE', 'TEMP_1', 'EDA_1','BVP_1','HR_1','IBI_1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4069df68-7bf6-4ea2-93f3-0a8f03021da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19200, 1, 7])\n"
     ]
    }
   ],
   "source": [
    "#Converting data to PyTorch tensors.\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "X_train_tensor = X_train_tensor.unsqueeze(1)\n",
    "X_test_tensor = X_test_tensor.unsqueeze(1)\n",
    "print(X_train_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b973def-b885-4321-abb0-7be7ae58cfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/400], Loss: 3.5990\n",
      "Epoch [2/400], Loss: 3.5966\n",
      "Epoch [3/400], Loss: 3.5943\n",
      "Epoch [4/400], Loss: 3.5919\n",
      "Epoch [5/400], Loss: 3.5896\n",
      "Epoch [6/400], Loss: 3.5872\n",
      "Epoch [7/400], Loss: 3.5849\n",
      "Epoch [8/400], Loss: 3.5825\n",
      "Epoch [9/400], Loss: 3.5801\n",
      "Epoch [10/400], Loss: 3.5777\n",
      "Epoch [11/400], Loss: 3.5753\n",
      "Epoch [12/400], Loss: 3.5729\n",
      "Epoch [13/400], Loss: 3.5704\n",
      "Epoch [14/400], Loss: 3.5679\n",
      "Epoch [15/400], Loss: 3.5654\n",
      "Epoch [16/400], Loss: 3.5629\n",
      "Epoch [17/400], Loss: 3.5603\n",
      "Epoch [18/400], Loss: 3.5576\n",
      "Epoch [19/400], Loss: 3.5550\n",
      "Epoch [20/400], Loss: 3.5522\n",
      "Epoch [21/400], Loss: 3.5495\n",
      "Epoch [22/400], Loss: 3.5466\n",
      "Epoch [23/400], Loss: 3.5438\n",
      "Epoch [24/400], Loss: 3.5408\n",
      "Epoch [25/400], Loss: 3.5379\n",
      "Epoch [26/400], Loss: 3.5348\n",
      "Epoch [27/400], Loss: 3.5317\n",
      "Epoch [28/400], Loss: 3.5285\n",
      "Epoch [29/400], Loss: 3.5253\n",
      "Epoch [30/400], Loss: 3.5220\n",
      "Epoch [31/400], Loss: 3.5187\n",
      "Epoch [32/400], Loss: 3.5152\n",
      "Epoch [33/400], Loss: 3.5117\n",
      "Epoch [34/400], Loss: 3.5082\n",
      "Epoch [35/400], Loss: 3.5045\n",
      "Epoch [36/400], Loss: 3.5008\n",
      "Epoch [37/400], Loss: 3.4970\n",
      "Epoch [38/400], Loss: 3.4932\n",
      "Epoch [39/400], Loss: 3.4893\n",
      "Epoch [40/400], Loss: 3.4853\n",
      "Epoch [41/400], Loss: 3.4812\n",
      "Epoch [42/400], Loss: 3.4771\n",
      "Epoch [43/400], Loss: 3.4729\n",
      "Epoch [44/400], Loss: 3.4686\n",
      "Epoch [45/400], Loss: 3.4643\n",
      "Epoch [46/400], Loss: 3.4599\n",
      "Epoch [47/400], Loss: 3.4555\n",
      "Epoch [48/400], Loss: 3.4510\n",
      "Epoch [49/400], Loss: 3.4464\n",
      "Epoch [50/400], Loss: 3.4418\n",
      "Epoch [51/400], Loss: 3.4371\n",
      "Epoch [52/400], Loss: 3.4324\n",
      "Epoch [53/400], Loss: 3.4276\n",
      "Epoch [54/400], Loss: 3.4228\n",
      "Epoch [55/400], Loss: 3.4179\n",
      "Epoch [56/400], Loss: 3.4130\n",
      "Epoch [57/400], Loss: 3.4081\n",
      "Epoch [58/400], Loss: 3.4031\n",
      "Epoch [59/400], Loss: 3.3981\n",
      "Epoch [60/400], Loss: 3.3931\n",
      "Epoch [61/400], Loss: 3.3880\n",
      "Epoch [62/400], Loss: 3.3829\n",
      "Epoch [63/400], Loss: 3.3778\n",
      "Epoch [64/400], Loss: 3.3727\n",
      "Epoch [65/400], Loss: 3.3676\n",
      "Epoch [66/400], Loss: 3.3625\n",
      "Epoch [67/400], Loss: 3.3573\n",
      "Epoch [68/400], Loss: 3.3522\n",
      "Epoch [69/400], Loss: 3.3471\n",
      "Epoch [70/400], Loss: 3.3420\n",
      "Epoch [71/400], Loss: 3.3369\n",
      "Epoch [72/400], Loss: 3.3318\n",
      "Epoch [73/400], Loss: 3.3268\n",
      "Epoch [74/400], Loss: 3.3217\n",
      "Epoch [75/400], Loss: 3.3167\n",
      "Epoch [76/400], Loss: 3.3118\n",
      "Epoch [77/400], Loss: 3.3068\n",
      "Epoch [78/400], Loss: 3.3019\n",
      "Epoch [79/400], Loss: 3.2971\n",
      "Epoch [80/400], Loss: 3.2923\n",
      "Epoch [81/400], Loss: 3.2876\n",
      "Epoch [82/400], Loss: 3.2829\n",
      "Epoch [83/400], Loss: 3.2782\n",
      "Epoch [84/400], Loss: 3.2736\n",
      "Epoch [85/400], Loss: 3.2691\n",
      "Epoch [86/400], Loss: 3.2647\n",
      "Epoch [87/400], Loss: 3.2603\n",
      "Epoch [88/400], Loss: 3.2559\n",
      "Epoch [89/400], Loss: 3.2517\n",
      "Epoch [90/400], Loss: 3.2475\n",
      "Epoch [91/400], Loss: 3.2434\n",
      "Epoch [92/400], Loss: 3.2393\n",
      "Epoch [93/400], Loss: 3.2354\n",
      "Epoch [94/400], Loss: 3.2315\n",
      "Epoch [95/400], Loss: 3.2276\n",
      "Epoch [96/400], Loss: 3.2239\n",
      "Epoch [97/400], Loss: 3.2202\n",
      "Epoch [98/400], Loss: 3.2166\n",
      "Epoch [99/400], Loss: 3.2131\n",
      "Epoch [100/400], Loss: 3.2097\n",
      "Epoch [101/400], Loss: 3.2063\n",
      "Epoch [102/400], Loss: 3.2030\n",
      "Epoch [103/400], Loss: 3.1998\n",
      "Epoch [104/400], Loss: 3.1967\n",
      "Epoch [105/400], Loss: 3.1936\n",
      "Epoch [106/400], Loss: 3.1906\n",
      "Epoch [107/400], Loss: 3.1877\n",
      "Epoch [108/400], Loss: 3.1848\n",
      "Epoch [109/400], Loss: 3.1821\n",
      "Epoch [110/400], Loss: 3.1794\n",
      "Epoch [111/400], Loss: 3.1767\n",
      "Epoch [112/400], Loss: 3.1741\n",
      "Epoch [113/400], Loss: 3.1716\n",
      "Epoch [114/400], Loss: 3.1692\n",
      "Epoch [115/400], Loss: 3.1668\n",
      "Epoch [116/400], Loss: 3.1645\n",
      "Epoch [117/400], Loss: 3.1623\n",
      "Epoch [118/400], Loss: 3.1601\n",
      "Epoch [119/400], Loss: 3.1580\n",
      "Epoch [120/400], Loss: 3.1559\n",
      "Epoch [121/400], Loss: 3.1539\n",
      "Epoch [122/400], Loss: 3.1519\n",
      "Epoch [123/400], Loss: 3.1500\n",
      "Epoch [124/400], Loss: 3.1482\n",
      "Epoch [125/400], Loss: 3.1464\n",
      "Epoch [126/400], Loss: 3.1446\n",
      "Epoch [127/400], Loss: 3.1429\n",
      "Epoch [128/400], Loss: 3.1413\n",
      "Epoch [129/400], Loss: 3.1397\n",
      "Epoch [130/400], Loss: 3.1381\n",
      "Epoch [131/400], Loss: 3.1366\n",
      "Epoch [132/400], Loss: 3.1352\n",
      "Epoch [133/400], Loss: 3.1337\n",
      "Epoch [134/400], Loss: 3.1324\n",
      "Epoch [135/400], Loss: 3.1310\n",
      "Epoch [136/400], Loss: 3.1297\n",
      "Epoch [137/400], Loss: 3.1284\n",
      "Epoch [138/400], Loss: 3.1272\n",
      "Epoch [139/400], Loss: 3.1260\n",
      "Epoch [140/400], Loss: 3.1249\n",
      "Epoch [141/400], Loss: 3.1237\n",
      "Epoch [142/400], Loss: 3.1226\n",
      "Epoch [143/400], Loss: 3.1216\n",
      "Epoch [144/400], Loss: 3.1205\n",
      "Epoch [145/400], Loss: 3.1195\n",
      "Epoch [146/400], Loss: 3.1186\n",
      "Epoch [147/400], Loss: 3.1176\n",
      "Epoch [148/400], Loss: 3.1167\n",
      "Epoch [149/400], Loss: 3.1158\n",
      "Epoch [150/400], Loss: 3.1150\n",
      "Epoch [151/400], Loss: 3.1141\n",
      "Epoch [152/400], Loss: 3.1133\n",
      "Epoch [153/400], Loss: 3.1125\n",
      "Epoch [154/400], Loss: 3.1118\n",
      "Epoch [155/400], Loss: 3.1110\n",
      "Epoch [156/400], Loss: 3.1103\n",
      "Epoch [157/400], Loss: 3.1096\n",
      "Epoch [158/400], Loss: 3.1089\n",
      "Epoch [159/400], Loss: 3.1083\n",
      "Epoch [160/400], Loss: 3.1076\n",
      "Epoch [161/400], Loss: 3.1070\n",
      "Epoch [162/400], Loss: 3.1064\n",
      "Epoch [163/400], Loss: 3.1058\n",
      "Epoch [164/400], Loss: 3.1052\n",
      "Epoch [165/400], Loss: 3.1047\n",
      "Epoch [166/400], Loss: 3.1041\n",
      "Epoch [167/400], Loss: 3.1036\n",
      "Epoch [168/400], Loss: 3.1031\n",
      "Epoch [169/400], Loss: 3.1026\n",
      "Epoch [170/400], Loss: 3.1021\n",
      "Epoch [171/400], Loss: 3.1016\n",
      "Epoch [172/400], Loss: 3.1012\n",
      "Epoch [173/400], Loss: 3.1007\n",
      "Epoch [174/400], Loss: 3.1003\n",
      "Epoch [175/400], Loss: 3.0998\n",
      "Epoch [176/400], Loss: 3.0994\n",
      "Epoch [177/400], Loss: 3.0990\n",
      "Epoch [178/400], Loss: 3.0986\n",
      "Epoch [179/400], Loss: 3.0982\n",
      "Epoch [180/400], Loss: 3.0978\n",
      "Epoch [181/400], Loss: 3.0975\n",
      "Epoch [182/400], Loss: 3.0971\n",
      "Epoch [183/400], Loss: 3.0968\n",
      "Epoch [184/400], Loss: 3.0964\n",
      "Epoch [185/400], Loss: 3.0961\n",
      "Epoch [186/400], Loss: 3.0958\n",
      "Epoch [187/400], Loss: 3.0954\n",
      "Epoch [188/400], Loss: 3.0951\n",
      "Epoch [189/400], Loss: 3.0948\n",
      "Epoch [190/400], Loss: 3.0945\n",
      "Epoch [191/400], Loss: 3.0942\n",
      "Epoch [192/400], Loss: 3.0939\n",
      "Epoch [193/400], Loss: 3.0936\n",
      "Epoch [194/400], Loss: 3.0934\n",
      "Epoch [195/400], Loss: 3.0931\n",
      "Epoch [196/400], Loss: 3.0928\n",
      "Epoch [197/400], Loss: 3.0926\n",
      "Epoch [198/400], Loss: 3.0923\n",
      "Epoch [199/400], Loss: 3.0921\n",
      "Epoch [200/400], Loss: 3.0918\n",
      "Epoch [201/400], Loss: 3.0916\n",
      "Epoch [202/400], Loss: 3.0914\n",
      "Epoch [203/400], Loss: 3.0911\n",
      "Epoch [204/400], Loss: 3.0909\n",
      "Epoch [205/400], Loss: 3.0907\n",
      "Epoch [206/400], Loss: 3.0905\n",
      "Epoch [207/400], Loss: 3.0903\n",
      "Epoch [208/400], Loss: 3.0901\n",
      "Epoch [209/400], Loss: 3.0899\n",
      "Epoch [210/400], Loss: 3.0897\n",
      "Epoch [211/400], Loss: 3.0895\n",
      "Epoch [212/400], Loss: 3.0893\n",
      "Epoch [213/400], Loss: 3.0891\n",
      "Epoch [214/400], Loss: 3.0889\n",
      "Epoch [215/400], Loss: 3.0887\n",
      "Epoch [216/400], Loss: 3.0885\n",
      "Epoch [217/400], Loss: 3.0884\n",
      "Epoch [218/400], Loss: 3.0882\n",
      "Epoch [219/400], Loss: 3.0880\n",
      "Epoch [220/400], Loss: 3.0879\n",
      "Epoch [221/400], Loss: 3.0877\n",
      "Epoch [222/400], Loss: 3.0875\n",
      "Epoch [223/400], Loss: 3.0874\n",
      "Epoch [224/400], Loss: 3.0872\n",
      "Epoch [225/400], Loss: 3.0871\n",
      "Epoch [226/400], Loss: 3.0869\n",
      "Epoch [227/400], Loss: 3.0868\n",
      "Epoch [228/400], Loss: 3.0866\n",
      "Epoch [229/400], Loss: 3.0865\n",
      "Epoch [230/400], Loss: 3.0864\n",
      "Epoch [231/400], Loss: 3.0862\n",
      "Epoch [232/400], Loss: 3.0861\n",
      "Epoch [233/400], Loss: 3.0859\n",
      "Epoch [234/400], Loss: 3.0858\n",
      "Epoch [235/400], Loss: 3.0857\n",
      "Epoch [236/400], Loss: 3.0856\n",
      "Epoch [237/400], Loss: 3.0854\n",
      "Epoch [238/400], Loss: 3.0853\n",
      "Epoch [239/400], Loss: 3.0852\n",
      "Epoch [240/400], Loss: 3.0851\n",
      "Epoch [241/400], Loss: 3.0850\n",
      "Epoch [242/400], Loss: 3.0848\n",
      "Epoch [243/400], Loss: 3.0847\n",
      "Epoch [244/400], Loss: 3.0846\n",
      "Epoch [245/400], Loss: 3.0845\n",
      "Epoch [246/400], Loss: 3.0844\n",
      "Epoch [247/400], Loss: 3.0843\n",
      "Epoch [248/400], Loss: 3.0842\n",
      "Epoch [249/400], Loss: 3.0841\n",
      "Epoch [250/400], Loss: 3.0840\n",
      "Epoch [251/400], Loss: 3.0839\n",
      "Epoch [252/400], Loss: 3.0838\n",
      "Epoch [253/400], Loss: 3.0837\n",
      "Epoch [254/400], Loss: 3.0836\n",
      "Epoch [255/400], Loss: 3.0835\n",
      "Epoch [256/400], Loss: 3.0834\n",
      "Epoch [257/400], Loss: 3.0833\n",
      "Epoch [258/400], Loss: 3.0832\n",
      "Epoch [259/400], Loss: 3.0831\n",
      "Epoch [260/400], Loss: 3.0831\n",
      "Epoch [261/400], Loss: 3.0830\n",
      "Epoch [262/400], Loss: 3.0829\n",
      "Epoch [263/400], Loss: 3.0828\n",
      "Epoch [264/400], Loss: 3.0827\n",
      "Epoch [265/400], Loss: 3.0826\n",
      "Epoch [266/400], Loss: 3.0825\n",
      "Epoch [267/400], Loss: 3.0825\n",
      "Epoch [268/400], Loss: 3.0824\n",
      "Epoch [269/400], Loss: 3.0823\n",
      "Epoch [270/400], Loss: 3.0822\n",
      "Epoch [271/400], Loss: 3.0822\n",
      "Epoch [272/400], Loss: 3.0821\n",
      "Epoch [273/400], Loss: 3.0820\n",
      "Epoch [274/400], Loss: 3.0819\n",
      "Epoch [275/400], Loss: 3.0819\n",
      "Epoch [276/400], Loss: 3.0818\n",
      "Epoch [277/400], Loss: 3.0817\n",
      "Epoch [278/400], Loss: 3.0816\n",
      "Epoch [279/400], Loss: 3.0816\n",
      "Epoch [280/400], Loss: 3.0815\n",
      "Epoch [281/400], Loss: 3.0814\n",
      "Epoch [282/400], Loss: 3.0814\n",
      "Epoch [283/400], Loss: 3.0813\n",
      "Epoch [284/400], Loss: 3.0812\n",
      "Epoch [285/400], Loss: 3.0812\n",
      "Epoch [286/400], Loss: 3.0811\n",
      "Epoch [287/400], Loss: 3.0811\n",
      "Epoch [288/400], Loss: 3.0810\n",
      "Epoch [289/400], Loss: 3.0809\n",
      "Epoch [290/400], Loss: 3.0809\n",
      "Epoch [291/400], Loss: 3.0808\n",
      "Epoch [292/400], Loss: 3.0807\n",
      "Epoch [293/400], Loss: 3.0807\n",
      "Epoch [294/400], Loss: 3.0806\n",
      "Epoch [295/400], Loss: 3.0806\n",
      "Epoch [296/400], Loss: 3.0805\n",
      "Epoch [297/400], Loss: 3.0805\n",
      "Epoch [298/400], Loss: 3.0804\n",
      "Epoch [299/400], Loss: 3.0804\n",
      "Epoch [300/400], Loss: 3.0803\n",
      "Epoch [301/400], Loss: 3.0802\n",
      "Epoch [302/400], Loss: 3.0802\n",
      "Epoch [303/400], Loss: 3.0801\n",
      "Epoch [304/400], Loss: 3.0801\n",
      "Epoch [305/400], Loss: 3.0800\n",
      "Epoch [306/400], Loss: 3.0800\n",
      "Epoch [307/400], Loss: 3.0799\n",
      "Epoch [308/400], Loss: 3.0799\n",
      "Epoch [309/400], Loss: 3.0798\n",
      "Epoch [310/400], Loss: 3.0798\n",
      "Epoch [311/400], Loss: 3.0797\n",
      "Epoch [312/400], Loss: 3.0797\n",
      "Epoch [313/400], Loss: 3.0796\n",
      "Epoch [314/400], Loss: 3.0796\n",
      "Epoch [315/400], Loss: 3.0795\n",
      "Epoch [316/400], Loss: 3.0795\n",
      "Epoch [317/400], Loss: 3.0795\n",
      "Epoch [318/400], Loss: 3.0794\n",
      "Epoch [319/400], Loss: 3.0794\n",
      "Epoch [320/400], Loss: 3.0793\n",
      "Epoch [321/400], Loss: 3.0793\n",
      "Epoch [322/400], Loss: 3.0792\n",
      "Epoch [323/400], Loss: 3.0792\n",
      "Epoch [324/400], Loss: 3.0791\n",
      "Epoch [325/400], Loss: 3.0791\n",
      "Epoch [326/400], Loss: 3.0791\n",
      "Epoch [327/400], Loss: 3.0790\n",
      "Epoch [328/400], Loss: 3.0790\n",
      "Epoch [329/400], Loss: 3.0789\n",
      "Epoch [330/400], Loss: 3.0789\n",
      "Epoch [331/400], Loss: 3.0789\n",
      "Epoch [332/400], Loss: 3.0788\n",
      "Epoch [333/400], Loss: 3.0788\n",
      "Epoch [334/400], Loss: 3.0787\n",
      "Epoch [335/400], Loss: 3.0787\n",
      "Epoch [336/400], Loss: 3.0787\n",
      "Epoch [337/400], Loss: 3.0786\n",
      "Epoch [338/400], Loss: 3.0786\n",
      "Epoch [339/400], Loss: 3.0785\n",
      "Epoch [340/400], Loss: 3.0785\n",
      "Epoch [341/400], Loss: 3.0785\n",
      "Epoch [342/400], Loss: 3.0784\n",
      "Epoch [343/400], Loss: 3.0784\n",
      "Epoch [344/400], Loss: 3.0784\n",
      "Epoch [345/400], Loss: 3.0783\n",
      "Epoch [346/400], Loss: 3.0783\n",
      "Epoch [347/400], Loss: 3.0783\n",
      "Epoch [348/400], Loss: 3.0782\n",
      "Epoch [349/400], Loss: 3.0782\n",
      "Epoch [350/400], Loss: 3.0782\n",
      "Epoch [351/400], Loss: 3.0781\n",
      "Epoch [352/400], Loss: 3.0781\n",
      "Epoch [353/400], Loss: 3.0781\n",
      "Epoch [354/400], Loss: 3.0780\n",
      "Epoch [355/400], Loss: 3.0780\n",
      "Epoch [356/400], Loss: 3.0780\n",
      "Epoch [357/400], Loss: 3.0779\n",
      "Epoch [358/400], Loss: 3.0779\n",
      "Epoch [359/400], Loss: 3.0779\n",
      "Epoch [360/400], Loss: 3.0778\n",
      "Epoch [361/400], Loss: 3.0778\n",
      "Epoch [362/400], Loss: 3.0778\n",
      "Epoch [363/400], Loss: 3.0777\n",
      "Epoch [364/400], Loss: 3.0777\n",
      "Epoch [365/400], Loss: 3.0777\n",
      "Epoch [366/400], Loss: 3.0776\n",
      "Epoch [367/400], Loss: 3.0776\n",
      "Epoch [368/400], Loss: 3.0776\n",
      "Epoch [369/400], Loss: 3.0775\n",
      "Epoch [370/400], Loss: 3.0775\n",
      "Epoch [371/400], Loss: 3.0775\n",
      "Epoch [372/400], Loss: 3.0775\n",
      "Epoch [373/400], Loss: 3.0774\n",
      "Epoch [374/400], Loss: 3.0774\n",
      "Epoch [375/400], Loss: 3.0774\n",
      "Epoch [376/400], Loss: 3.0773\n",
      "Epoch [377/400], Loss: 3.0773\n",
      "Epoch [378/400], Loss: 3.0773\n",
      "Epoch [379/400], Loss: 3.0773\n",
      "Epoch [380/400], Loss: 3.0772\n",
      "Epoch [381/400], Loss: 3.0772\n",
      "Epoch [382/400], Loss: 3.0772\n",
      "Epoch [383/400], Loss: 3.0771\n",
      "Epoch [384/400], Loss: 3.0771\n",
      "Epoch [385/400], Loss: 3.0771\n",
      "Epoch [386/400], Loss: 3.0771\n",
      "Epoch [387/400], Loss: 3.0770\n",
      "Epoch [388/400], Loss: 3.0770\n",
      "Epoch [389/400], Loss: 3.0770\n",
      "Epoch [390/400], Loss: 3.0770\n",
      "Epoch [391/400], Loss: 3.0769\n",
      "Epoch [392/400], Loss: 3.0769\n",
      "Epoch [393/400], Loss: 3.0769\n",
      "Epoch [394/400], Loss: 3.0769\n",
      "Epoch [395/400], Loss: 3.0768\n",
      "Epoch [396/400], Loss: 3.0768\n",
      "Epoch [397/400], Loss: 3.0768\n",
      "Epoch [398/400], Loss: 3.0768\n",
      "Epoch [399/400], Loss: 3.0767\n",
      "Epoch [400/400], Loss: 3.0767\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MoodLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MoodLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim  # Store hidden_dim as a class attribute\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(1, batch_size, self.hidden_dim).requires_grad_()\n",
    "        out, _ = self.lstm(x, (h0.detach(), c0.detach()))  \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "#Setting model architecture.\n",
    "input_dim = 7  \n",
    "hidden_dim = 64\n",
    "output_dim = 36 #Number of classes in Emotion column.\n",
    "\n",
    "#Instantiating the model.\n",
    "model = MoodLSTM(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "#Defining loss function and Optimizer.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#Training the model.\n",
    "epochs = 400\n",
    "for epoch in range(epochs):\n",
    "    #Forward pass.\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    \n",
    "    #Backward pass.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, loss.item()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef3d4d73-7817-42c9-992a-cf4443d19aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 9.04%\n"
     ]
    }
   ],
   "source": [
    "#Testing the model for accuracy.\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "correct = (predicted == y_test_tensor).sum().item()\n",
    "total = y_test_tensor.size(0)\n",
    "accuracy = correct / total\n",
    "\n",
    "print('Accuracy on test data: {:.2f}%'.format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbafe60-2348-4ca3-820a-9abd49c8fe7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
